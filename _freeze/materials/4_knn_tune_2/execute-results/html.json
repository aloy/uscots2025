{
  "hash": "a9666068f9c62568713285f3a31db971",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Parameter Tuning: A KNN Example\"\nauthor: \"Jaime Davila and Adam Loy\"\ndate: \"Last updated:\"\noutput:\n  pdf_document: default\n  html_document: default\neditor_options:\n  chunk_output_type: console\n---\n\n\n\n\n\n## Intended Learning Outcomes\n\nBy the end of this activity, you will be able to:\n\n* Tune the parameters of a KNN model using cross validation\n* Explore the results of the tuning process\n* Finalize a KNN model workflow with the selected parameter values\n\n## Overview\n\nNow that you know how to fit KNN models and how to conduct cross validation to estimate their out-of-bag performance, it's time to explore how to tune the parameters of a KNN model using the functionality of the {tidymodels} ecosystem.\n\nIn this activity we'll revisit the Scooby Doo data set and use it to build a KNN model that predicts whether a monster is real or not. Our goal is to select the \"best\" value for the number of neighbors,`k`. To begin, let's load in the data and create our training and testing splits:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscooby <- readr::read_csv(\"scooby.csv\", col_types=\"ddfcd\")\nset.seed(1234)\n\nscooby_split <- initial_split(scooby, \n                              prop = 0.75, \n                              strata = monster_real)\nscooby_train <- training(scooby_split)\nscooby_test <- testing(scooby_split)\n```\n:::\n\n\n\n## Tuning a KNN Model\n\nTo tune a model, we must first define our model, which includes specifying the recipe and the model specification, and then combining these components into a workflow. There is one key change required, however, to perform parameter tuning: we need to use the `tune()` function to indicate which parameter we want to tune in our model specification. In the case of KNN, we set `neighbors = tune()` rather than setting it to a specific value.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Recipe definition\nknn_recipe <- recipe(monster_real ~ imdb + year_aired, data = scooby_train) |>\n  step_normalize(all_numeric_predictors()) |>\n  step_naomit(all_predictors())\n\n# Model specification\nknn_spec <- nearest_neighbor(neighbors = tune()) |>\n  set_mode(\"classification\") |>\n  set_engine(\"kknn\")\n\n# Workflow definition\nknn_wflow <- workflow() |>\n  add_recipe(knn_recipe) |>\n  add_model(knn_spec)\n```\n:::\n\n\n\nNext, we need to create our cross-validation splits. In this example, let's use 5-fold cross-validation.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cross-validation splits\nscooby_folds <- vfold_cv(scooby_train, v = 5, strata = monster_real)\n```\n:::\n\n\n\nWe also need to create a grid of values for the `neighbors` parameter, for example from 1 to 21 (odds only), using the `grid_regular()` function. We then pass this grid to the `tune_grid()` function along with our workflow and cross-validation folds. When tuning a single parameter you could also create a tibble with the values you want to test, but using `grid_regular()` is a convenient way to generate a range of values and makes tuning multiple parameters easier.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk_grid <- grid_regular(neighbors(range = c(1, 21)), levels = 10)\n```\n:::\n\n\n\nNote: {tidymodels} does have functionality to automatically generate candidate parameter grids for many models; however, it's better to manually specify the values when you are first learning to reinforce your understanding of the parameters and their impact on model performance.\n\n\nWith the grid defined, we can now perform the tuning process by passing in the workflow, the cross-validation folds, and the grid of values to the `tune_grid()` function. We also specify the metrics we will use to evaluate the model's performance, such as accuracy, sensitivity, and specificity. Recall that we use the `metric_set()` to combine multiple metrics into a single object that can be passed to the tuning function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscooby_tune <- tune_grid(\n  knn_wflow,\n  resamples = scooby_folds,\n  grid = k_grid,\n  metrics = metric_set(accuracy, sens, spec)\n)\n```\n:::\n\n\n\nTo collect the results of the tuning process, we use the `collect_metrics()` function. This returns a tibble with the performance metrics for each value of `k` that was explored.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(scooby_tune_results <- collect_metrics(scooby_tune))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 30 × 7\n   neighbors .metric  .estimator  mean     n std_err .config              \n       <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1         1 accuracy binary     0.835     5  0.0186 Preprocessor1_Model01\n 2         1 sens     binary     0.917     5  0.0253 Preprocessor1_Model01\n 3         1 spec     binary     0.549     5  0.0267 Preprocessor1_Model01\n 4         3 accuracy binary     0.827     5  0.0172 Preprocessor1_Model02\n 5         3 sens     binary     0.914     5  0.0225 Preprocessor1_Model02\n 6         3 spec     binary     0.525     5  0.0386 Preprocessor1_Model02\n 7         5 accuracy binary     0.843     5  0.0194 Preprocessor1_Model03\n 8         5 sens     binary     0.918     5  0.0199 Preprocessor1_Model03\n 9         5 spec     binary     0.585     5  0.0391 Preprocessor1_Model03\n10         7 accuracy binary     0.875     5  0.0121 Preprocessor1_Model04\n# ℹ 20 more rows\n```\n\n\n:::\n:::\n\n\n\n**Your turn 1**: Explore the results of 5-fold cross validation for the various values of `k`. What is the best value of `k` based on the accuracy metric? How does this value compare to the sensitivity and specificity metrics? \n\n\nIt can be tedious to explore the table of results manually, particularly if you have many parameters to tune. To quickly visualize the results we use the `autoplot()` function from the {tune} package. This function will create a plot of the performance metrics for each value of our parameters, allowing us to quickly identify the best performing model.\n\n\n**Your turn 2**: Use the `autoplot()` function to visualize the results of the tuning process. What does the plot tell you about the relationship between the number of neighbors and the model's performance?\n\n\n\n\nIf your goal is to select the \"best\" value of `k`, you can use the `select_best()` function to extract the best performing parameter value based on a specific metric. For example, if you want to select the best value of `k` based on accuracy, you can do the following:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_k <- select_best(scooby_tune, metric = \"accuracy\")\n```\n:::\n\n\n\nThis will return a tibble with the best value of `k` and its corresponding accuracy.\n\n**Your turn 3**: What is the best value of `k` based on sensitivity? For specificity?\n\nOnce you have determined what value of `k` to use, you can **finalize** your model.\n\n\n## Other 1 SE for optimizing parameters\n\nOften there are many values of `k` that yield similar performance. In these cases, it can be beneficial to select a model that is easier to interpret and less likely to overfit the training data. One common approach is the \"one standard error\" rule, which selects the simplest model whose performance is within one standard error of the best performing model. This can be done using the `select_by_one_std_err()` function, which takes the results of `tune_grid()`, the name of the performance metric (as a character string), and the parameter(s) to optimize (unquoted).\n\n\n\n\n**Your turn 4**: What value of `k` is selected using the one standard error rule based on sensitivity? For specificity?\n\n**Your turn 5**: Finalize the model using the value of `k` selected by the one standard error rule based on accuracy. How does this model perform on the test data compared to the model selected by the best accuracy?\n\n## {usemodels} for parameter optimization\n\nIt can be rather tedious to type out all the code required to tune a starting from the recipe. The {usemodels} package implements model skeletons for a few commonly used models, including KNN. Given a formula and data set it will create the recipe, model specification, and workflow for you. It also sets the framework for tuning the model with helpful reminders of the components you should think carefully about.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(usemodels)\nuse_kknn(monster_real ~ imdb + year_aired, data = scooby)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nkknn_recipe <- \n  recipe(formula = monster_real ~ imdb + year_aired, data = scooby) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_numeric_predictors()) \n\nkknn_spec <- \n  nearest_neighbor(neighbors = tune(), weight_func = tune()) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"kknn\") \n\nkknn_workflow <- \n  workflow() %>% \n  add_recipe(kknn_recipe) %>% \n  add_model(kknn_spec) \n\nset.seed(68778)\nkknn_tune <-\n  tune_grid(kknn_workflow, resamples = stop(\"add your rsample object\"), grid = stop(\"add number of candidate points\"))\n```\n\n\n:::\n:::\n\n\n\n\n**Your turn 6**: What two objects do you need to define in order to run the tuning process using the {usemodels} package? What alterations to the code are needed?\n\n# Functions used\n\n* `grid_regular()`: Generates a regular grid of parameter values for tuning.\n* `tune_grid()`: Performs tuning of a model using cross-validation and a grid of parameter values.\n* `autoplot()`: Visualizes the results of the tuning process, showing performance metrics for different parameter values.\n* `select_best()`, `select_by_one_std_err()`: Functions to select the best parameter values based on a specific metric or the one standard error rule.\n* `finalize_workflow()`: Finalizes a workflow with the best parameter values selected from tuning.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
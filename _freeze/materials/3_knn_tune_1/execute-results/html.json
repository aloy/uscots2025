{
  "hash": "bdfa1187fe7bb0c4368aaff4380a0e52",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Optimizing parameters using resampling\"\nauthor: \"Jaime Davila and Adam Loy\"\noutput:\n  pdf_document: default\n  html_document: default\ndate: \"2025-06-06\"\neditor_options:\n  chunk_output_type: console\n---\n\n\n\n\n\n# Intending Learning Outcomes\n\n* Uses visualization to see the effect of K in a KNN classification.\n* Recognize the need for cross-validation and is able to construct and interact with cross-validation datasets.\n* Uses cross-validation to evaluate performance metrics across multiple models.\n\n# Dataset\n\nTo start, let's load the Scooby-Doo dataset and quickly divide it into training and testing sets:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscooby <- read_csv(\"scooby.csv\", col_types = \"ddfcd\")\n\nset.seed(1234)\nscooby_split <- initial_split(scooby, prop = 0.75, strata = monster_real)\nscooby_train <- training(scooby_split)\nscooby_test <- testing(scooby_split)\n```\n:::\n\n\n\n# Visualizing different KNN models\n\nLet's visualize an approximate decision boundary of a KNN model as we change the value of `k`. To do this, we proceed in two steps:\n\n* We define the function `create_knn_wf()` that receives `k` and a `train`ing (Scooby-Doo) dataset and creates a workflow for a KNN model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncreate_knn_wf <- function (k, train) {\n  # Model specification\n  knn_spec <- nearest_neighbor(neighbors = k) |>\n    set_mode(\"classification\") |>\n    set_engine(\"kknn\")\n  \n  # Recipe definition\n  knn_recipe <- recipe(monster_real ~ imdb + year_aired, \n                       data = train) |>\n   step_normalize(all_numeric_predictors()) |>\n   step_naomit(all_predictors())\n\n  # Workflow definition\n  knn_wflow <- workflow() |>\n    add_recipe(knn_recipe) |>\n    add_model(knn_spec)\n}\n```\n:::\n\n\n\n* We define the function `plot_boundary()` to create a grid with range of valid values for `year_aired` and `imdb` and evaluates the specified `model`, producing a visualization of the decision boundary\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_boundary <- function (model) {\n  scooby_grid <- expand_grid (year_aired=1969:2020, imdb=seq(4,10,by=.1))\n  augment(model, scooby_grid)|>\n    ggplot(aes(year_aired, imdb, fill = .pred_class)) +\n    geom_raster()+\n    labs(title = \"Scooby Doo Monsters\",\n       x = \"Year aired\",\n       y = \"IMDB rating\",\n       fill = \"Monster type\")\n\n}\n```\n:::\n\n\n\n\n**Your turn 1**: Using `k = 3`, plot the decision boundary of your KNN model by leveraging `create_knn_wf()` and `plot_boundary()`. Remember that `create_knn_wf()` creates a workflow, which you still need to **fit** in order to create a model.\n\n\n**Your turn 2**: Plot your decision boundary for k = 10, 50, 150. Discuss with your neighbor the effect changing `k` has on your decision boundary.\n\n\n# Evaluating models using cross-validation\n\nWe would like to find the optimal value of k for our KNN model. One way to do this would be to train different models using different values of k and then evaluate the performance of each model using our testing dataset. The main problem with this approach is that we are repeatedly looking at our testing dataset which can result in *overfitting* our model.\n\nHowever, this idea can be refined by dividing our training dataset into multiple training/testing datasets. A good explanation of how cross-validation works is found in [Introduction to Data Science](http://rafalab.dfci.harvard.edu/dsbook-part-2/ml/resampling-methods.html#cross-validation).\n\n## Creating a 10-fold cross-validation\n\nLet's start creating a 10-fold cross-validation data set called `scooby_folds` by leveraging the function `vfold_cv()`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\n(scooby_folds <- vfold_cv(scooby_train, v = 10))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits           id    \n   <list>           <chr> \n 1 <split [337/38]> Fold01\n 2 <split [337/38]> Fold02\n 3 <split [337/38]> Fold03\n 4 <split [337/38]> Fold04\n 5 <split [337/38]> Fold05\n 6 <split [338/37]> Fold06\n 7 <split [338/37]> Fold07\n 8 <split [338/37]> Fold08\n 9 <split [338/37]> Fold09\n10 <split [338/37]> Fold10\n```\n\n\n:::\n:::\n\n\n\n`scooby_folds` is a tibble with 2 columns (`splits` and `id`) and 10 rows. The `id` column gives us the name of the corresponding fold (or resample), while `splits` has a more complex datatype containing the training/testing dataset. In the next exercise we will learn a few more details about it.\n\n**Your turn 3**:\n\n* Use the function `get_rsplit()` (make sure to view the documentation using `?get_rsplit`) to get the second fold from `scooby_folds`, store the result in `scooby_fold_02`, and use the function `class()` (type) of `scooby_fold_02`. \n* Have you seen the class `rsplit` before? \n* What is the class of `scooby_split`? (Remember first code chunk in the `Dataset` section)\n\n\n\n**Your turn 4**: \n\n* Use the `testing()` and `training()` functions to extract the testing and training dataset from `scooby_fold_02`? \n* How big are your training/testing datasets? Does that correspond to the information from the second row of `scooby_folds`?\n\n\n## Testing our models using cross-validation\n\nLet's start by creating a KNN using `k = 3`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscooby_knn_wf1 <- create_knn_wf(3, scooby_train)\n```\n:::\n\n\n\nThe function `fit_resamples()` allows you to fit a workflow using the 10 different training/testing datasets that are stored in `scooby_fold`. Let's see it in action:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_resamples(scooby_knn_wf1, scooby_folds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Resampling results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits           id     .metrics         .notes          \n   <list>           <chr>  <list>           <list>          \n 1 <split [337/38]> Fold01 <tibble [3 × 4]> <tibble [0 × 3]>\n 2 <split [337/38]> Fold02 <tibble [3 × 4]> <tibble [0 × 3]>\n 3 <split [337/38]> Fold03 <tibble [3 × 4]> <tibble [0 × 3]>\n 4 <split [337/38]> Fold04 <tibble [3 × 4]> <tibble [0 × 3]>\n 5 <split [337/38]> Fold05 <tibble [3 × 4]> <tibble [0 × 3]>\n 6 <split [338/37]> Fold06 <tibble [3 × 4]> <tibble [0 × 3]>\n 7 <split [338/37]> Fold07 <tibble [3 × 4]> <tibble [0 × 3]>\n 8 <split [338/37]> Fold08 <tibble [3 × 4]> <tibble [0 × 3]>\n 9 <split [338/37]> Fold09 <tibble [3 × 4]> <tibble [0 × 3]>\n10 <split [338/37]> Fold10 <tibble [3 × 4]> <tibble [0 × 3]>\n```\n\n\n:::\n:::\n\n\n\nNotice that the output is 10 by 4 tibble, where three of the columns have nested tibbles inside. An easier way to interact with this dataset is by using the function `collect_metrics()` which summarizes some performance metrics across our cross-validation dataset:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_resamples(scooby_knn_wf1, scooby_folds) |>\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  <chr>       <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy    binary     0.848    10  0.0287 Preprocessor1_Model1\n2 brier_class binary     0.118    10  0.0215 Preprocessor1_Model1\n3 roc_auc     binary     0.857    10  0.0331 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\n\nIt seems however that the default metrics don't include specificity and sensitivity so we will create a new set of metrics using `metric_set`, which we add as a parameter in `fit_resamples()`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmulti_metric <- metric_set(accuracy, sens, spec)\nfit_resamples(scooby_knn_wf1, scooby_folds, \n              metrics = multi_metric) |>\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.848    10  0.0287 Preprocessor1_Model1\n2 sens     binary     0.934    10  0.0209 Preprocessor1_Model1\n3 spec     binary     0.552    10  0.0521 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\n\n**Your turn 5**: Using your 10-fold cross validations evaluate your accuracy, sensitivity and specificity for KNN models using k = 10, 50, 150. \n\n* Which value would you use to maximize accuracy? \n* How about sensitivity?\n\n\n# New {tidymodels} functions introduced\n\n* `vfold_cv()`: Creates a cross-validation dataset\n* `fit_resamples()`: Fits a workflow on a cross-validation (or a resampling) dataset\n* `collect_metrics()`: Creates a summary of the performance metrics after using `fit_resamples`\n* `metric_set()`: Allows to group specific performance metrics for use with `fit_resamples` \n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
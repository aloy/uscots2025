---
title: "Optimizing parameters using resampling"
author: "Jaime Davila and Adam Loy"
output:
  pdf_document: default
  html_document: default
date: "2025-06-06"
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
tidymodels_prefer(quiet = TRUE)
```

# Intending Learning Outcomes

* Uses visualization to see the effect of K in a KNN classification.
* Recognize the need for cross-validation and is able to construct and interact with cross-validation datasets.
* Uses cross-validation to evaluate performance metrics across multiple models.

# Dataset

To start, let's load the Scooby-Doo dataset and quickly divide it into training and testing sets:

```{r}
scooby <- read_csv("scooby.csv", col_types = "ddfcd")

set.seed(1234)
scooby_split <- initial_split(scooby, prop = 0.75, strata = monster_real)
scooby_train <- training(scooby_split)
scooby_test <- testing(scooby_split)
```

# Visualizing different KNN models

Let's visualize an approximate decision boundary of a KNN model as we change the value of `k`. To do this, we proceed in two steps:

* We define the function `create_knn_wf()` that receives `k` and a `train`ing (Scooby-Doo) dataset and creates a workflow for a KNN model

```{r}
create_knn_wf <- function (k, train) {
  # Model specification
  knn_spec <- nearest_neighbor(neighbors = k) |>
    set_mode("classification") |>
    set_engine("kknn")
  
  # Recipe definition
  knn_recipe <- recipe(monster_real ~ imdb + year_aired, 
                       data = train) |>
   step_normalize(all_numeric_predictors()) |>
   step_naomit(all_predictors())

  # Workflow definition
  knn_wflow <- workflow() |>
    add_recipe(knn_recipe) |>
    add_model(knn_spec)
}
```

* We define the function `plot_boundary()` to create a grid with range of valid values for `year_aired` and `imdb` and evaluates the specified `model`, producing a visualization of the decision boundary


```{r}
plot_boundary <- function (model) {
  scooby_grid <- expand_grid (year_aired=1969:2020, imdb=seq(4,10,by=.1))
  augment(model, scooby_grid)|>
    ggplot(aes(year_aired, imdb, fill = .pred_class)) +
    geom_raster()+
    labs(title = "Scooby Doo Monsters",
       x = "Year aired",
       y = "IMDB rating",
       fill = "Monster type")

}
```


**Your turn 1**: Using `k = 3`, plot the decision boundary of your KNN model by leveraging `create_knn_wf()` and `plot_boundary()`. Remember that `create_knn_wf()` creates a workflow, which you still need to **fit** in order to create a model.


**Your turn 2**: Plot your decision boundary for k = 10, 50, 150. Discuss with your neighbor the effect changing `k` has on your decision boundary.


# Evaluating models using cross-validation

We would like to find the optimal value of k for our KNN model. One way to do this would be to train different models using different values of k and then evaluate the performance of each model using our testing dataset. The main problem with this approach is that we are repeatedly looking at our testing dataset which can result in *overfitting* our model.

However, this idea can be refined by dividing our training dataset into multiple training/testing datasets. A good explanation of how cross-validation works is found in [Introduction to Data Science](http://rafalab.dfci.harvard.edu/dsbook-part-2/ml/resampling-methods.html#cross-validation).

## Creating a 10-fold cross-validation

Let's start creating a 10-fold cross-validation data set called `scooby_folds` by leveraging the function `vfold_cv()`


```{r}
set.seed(12345)
(scooby_folds <- vfold_cv(scooby_train, v = 10))
```

`scooby_folds` is a tibble with 2 columns (`splits` and `id`) and 10 rows. The `id` column gives us the name of the corresponding fold (or resample), while `splits` has a more complex datatype containing the training/testing dataset. In the next exercise we will learn a few more details about it.

**Your turn 3**:

* Use the function `get_rsplit()` (make sure to view the documentation using `?get_rsplit`) to get the second fold from `scooby_folds`, store the result in `scooby_fold_02`, and use the function `class()` (type) of `scooby_fold_02`. 
* Have you seen the class `rsplit` before? 
* What is the class of `scooby_split`? (Remember first code chunk in the `Dataset` section)



**Your turn 4**: 

* Use the `testing()` and `training()` functions to extract the testing and training dataset from `scooby_fold_02`? 
* How big are your training/testing datasets? Does that correspond to the information from the second row of `scooby_folds`?


## Testing our models using cross-validation

Let's start by creating a KNN using `k = 3`

```{r}
scooby_knn_wf1 <- create_knn_wf(3, scooby_train)
```

The function `fit_resamples()` allows you to fit a workflow using the 10 different training/testing datasets that are stored in `scooby_fold`. Let's see it in action:

```{r}
fit_resamples(scooby_knn_wf1, scooby_folds)
```

Notice that the output is 10 by 4 tibble, where three of the columns have nested tibbles inside. An easier way to interact with this dataset is by using the function `collect_metrics()` which summarizes some performance metrics across our cross-validation dataset:

```{r}
fit_resamples(scooby_knn_wf1, scooby_folds) |>
  collect_metrics()
```

It seems however that the default metrics don't include specificity and sensitivity so we will create a new set of metrics using `metric_set`, which we add as a parameter in `fit_resamples()`

```{r}
multi_metric <- metric_set(accuracy, sens, spec)
fit_resamples(scooby_knn_wf1, scooby_folds, 
              metrics = multi_metric) |>
  collect_metrics()
```

**Your turn 5**: Using your 10-fold cross validations evaluate your accuracy, sensitivity and specificity for KNN models using k = 10, 50, 150. 

* Which value would you use to maximize accuracy? 
* How about sensitivity?


# New {tidymodels} functions introduced

* `vfold_cv()`: Creates a cross-validation dataset
* `fit_resamples()`: Fits a workflow on a cross-validation (or a resampling) dataset
* `collect_metrics()`: Creates a summary of the performance metrics after using `fit_resamples`
* `metric_set()`: Allows to group specific performance metrics for use with `fit_resamples` 



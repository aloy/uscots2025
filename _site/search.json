[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Teaching statistical learning: An active approach using R and {tidymodels}",
    "section": "",
    "text": "Jaime Davila and Adam Loy\n\nJuly 15, 2025  1:00 – 4:15 pm  Location: Gerdin Business Building, Room 2464"
  },
  {
    "objectID": "index.html#before-the-workshop",
    "href": "index.html#before-the-workshop",
    "title": "Teaching statistical learning: An active approach using R and {tidymodels}",
    "section": "Before the workshop",
    "text": "Before the workshop\nBefore arriving, please make sure you have an up-to-date installation of R and RStudio, and install the following packages: {tidymodels}, {tidyverse}. You can use the below code chunk to do this.\n\ninstall.packages(c(\"tidymodels\", \"tidyverse\"))"
  },
  {
    "objectID": "index.html#agenda",
    "href": "index.html#agenda",
    "title": "Teaching statistical learning: An active approach using R and {tidymodels}",
    "section": "Agenda",
    "text": "Agenda\n\nIntroductions\nOverview of {tidymodels}\nActivity: KNN for regression/prediction\nActivity: KNN for classification\nBreak\nOverview of parameter tuning via cross validation and activity\nOverview of grid search and activity\nDebrief and Q&A"
  },
  {
    "objectID": "index.html#workshop-materials",
    "href": "index.html#workshop-materials",
    "title": "Teaching statistical learning: An active approach using R and {tidymodels}",
    "section": "Workshop materials",
    "text": "Workshop materials\nYou can access the materials either through the Google drive folder or the Github repo. Choose what you’re most comfortable with. We recommend creating a new RStudio project (.Rproj) for the workshop so that it’s easy to store the data and .Rmd files in the same directory.\n\nGoogle drive folder with all materials: https://tinyurl.com/USCOTS2025\nGithub repo"
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Teaching statistical learning: An active approach using R and {tidymodels}",
    "section": "Resources",
    "text": "Resources\nHere are additional resources that we found useful as we prepared to teach with {tidymodels}:\n\n{tidymodels} website: The official website for the {tidymodels} ecosystem\nTidy Modeling with R: Book by Max Kuhn and Julia Silge\nJulia Silge’s blog: Blog posts on various {tidymodels} topics\nTidy Tuesday"
  },
  {
    "objectID": "materials/1_knn_prediction.html",
    "href": "materials/1_knn_prediction.html",
    "title": "Prediction using KNN",
    "section": "",
    "text": "Understand how a KNN model works for prediction problems\nBecome familiar with the steps used in {tidymodels} to fit and evaluate a model.\nRecognize the need for using separate training and testing datasets."
  },
  {
    "objectID": "materials/1_knn_prediction.html#creating-testingtraining-datasets",
    "href": "materials/1_knn_prediction.html#creating-testingtraining-datasets",
    "title": "Prediction using KNN",
    "section": "Creating testing/training datasets",
    "text": "Creating testing/training datasets\nWe start by generating our training/testing datasets:\n\ntrain_mn_police_tbl &lt;- mn_police_tbl |&gt;\n  filter(year == 2016)\ntest_mn_police_tbl &lt;- mn_police_tbl |&gt;\n  filter(year == 2017)"
  },
  {
    "objectID": "materials/1_knn_prediction.html#knn-models",
    "href": "materials/1_knn_prediction.html#knn-models",
    "title": "Prediction using KNN",
    "section": "KNN models",
    "text": "KNN models\nFor our first prediction model we will use a “K Nearest Neighbors” (KNN) model. Prediction in a KNN model works by averaging the response variable across the \\(k\\) closest observations from your training dataset. Notice than in a KNN model we need to specify the value of \\(k\\).\nYour turn 3: With your neighbor, discuss how a KNN model would work to predict the number of incidents for week 20 using \\(k=3\\). Use R to calculate the model prediction."
  },
  {
    "objectID": "materials/1_knn_prediction.html#using-tidymodels-to-implement-a-knn-model",
    "href": "materials/1_knn_prediction.html#using-tidymodels-to-implement-a-knn-model",
    "title": "Prediction using KNN",
    "section": "Using {tidymodels} to implement a KNN model",
    "text": "Using {tidymodels} to implement a KNN model\nIn the following code we will introduce how to implement a KNN model using {tidymodels}. After showing the code, We will explain in detail:\n\n# Library setup\nlibrary(tidymodels)\ntidymodels_prefer(quiet = TRUE)\nlibrary(kknn)\n\n\n# Model specification\nkNear &lt;- 3\nknn_spec &lt;- nearest_neighbor(neighbors = kNear, \n                             weight_func =\"rectangular\") |&gt;\n  set_mode(\"regression\") |&gt;\n  set_engine(\"kknn\")\n\n\n# Recipe definition\nknn_recipe &lt;- recipe(tot ~ week, data=train_mn_police_tbl)\n\n# Workflow definition\nknn_wflow &lt;- workflow() |&gt;\n  add_recipe(knn_recipe) |&gt;\n  add_model(knn_spec)\n\n\n# Model training\nknn_model &lt;- fit(knn_wflow, train_mn_police_tbl)\n\nLet’s go into the details of how the above code works:\n\nLibrary setup: First, we tell R that we will be using the {tidymodels} framework, that we would like to resolve conflicts between {tidymodels} and other libraries without having warnings and that we will be using {kknn}, a library that implements the KNN model.\nModel specification: Next, we tell R to use a nearest_neighor() model (a KNN model), that we are setting our nearest-neighbor parameter to 3 and the weight function to rectangular (each observation weighs the same), that we are in the regression mode (as opposed to classification, more details on this in the upcoming classes), since we are predicting a numerical variable (tot is a number), and that we will be using the {kknn} library.\nRecipe definition: We declare response variable (tot) and the explanatory variable (week) in a recipe. Notice that in R the notation y ~ x means that y is your response variable and x is your explanatory variable.\nWorkflow definition: We are combine our model specification and recipe into a single unit called a workflow. Our workflow will be called knn_wflow\nModel training: Finally, we fit (or create) our model based on our training dataset.\n\nAlthough the syntax might strike you as verbose at first, the advantage of {tidymodels} is that we use the same syntax no matter what type of model we use.\nBefore moving on, take a look at knn_model object and notice some of the parameters that we used:\n\nknn_model\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nkknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(3,     data, 5), kernel = ~\"rectangular\")\n\nType of response variable: continuous\nminimal mean absolute error: 26.67308\nMinimal mean squared error: 1088.703\nBest kernel: rectangular\nBest k: 3"
  },
  {
    "objectID": "materials/1_knn_prediction.html#evaluating-our-knn-model",
    "href": "materials/1_knn_prediction.html#evaluating-our-knn-model",
    "title": "Prediction using KNN",
    "section": "Evaluating our KNN model",
    "text": "Evaluating our KNN model\nYour turn 4:  Run the following code and describe what you think augment() is doing. How many additional columns are added to test_mn_police_tbl and what do they mean? What are the type/dimensions of your output? Where in the output are themodel’s predictions? Check the documentation for augment and double-check your answers.\nYour turn 5: Use augment() on your testing dataset (test_mn_police_tbl) and create a plot illustrating the number of reports per week with your predictions superimposed. How well does your model predict the reported values in 2017?\nYour turn 6: Repeat the previous exercise but use your training dataset (train_mn_police_tbl) this time. How well does your model predict the reported values in 2016? Why do you think the model is doing better for 2016?\nYour turn 7: One way to measure the fit of a regression model is to calculate the Root Mean Square Error (RMSE), which can be interpreted as the standard deviation of the residuals (the difference between the model and the observed value). Notice that the RMSE has the same units as the response variable. Check the documentation for rmse() (in the console: ?rmse) and use it to calculate the RMSE of the model on the testing and training datasets. Are your results consistent with exercises 5 and 6?"
  },
  {
    "objectID": "materials/4_knn_tune_2.html",
    "href": "materials/4_knn_tune_2.html",
    "title": "Parameter Tuning: A KNN Example",
    "section": "",
    "text": "By the end of this activity, you will be able to:\n\nTune the parameters of a KNN model using cross validation\nExplore the results of the tuning process\nFinalize a KNN model workflow with the selected parameter values"
  },
  {
    "objectID": "materials/4_knn_tune_2.html#intended-learning-outcomes",
    "href": "materials/4_knn_tune_2.html#intended-learning-outcomes",
    "title": "Parameter Tuning: A KNN Example",
    "section": "",
    "text": "By the end of this activity, you will be able to:\n\nTune the parameters of a KNN model using cross validation\nExplore the results of the tuning process\nFinalize a KNN model workflow with the selected parameter values"
  },
  {
    "objectID": "materials/4_knn_tune_2.html#overview",
    "href": "materials/4_knn_tune_2.html#overview",
    "title": "Parameter Tuning: A KNN Example",
    "section": "Overview",
    "text": "Overview\nNow that you know how to fit KNN models and how to conduct cross validation to estimate their out-of-bag performance, it’s time to explore how to tune the parameters of a KNN model using the functionality of the {tidymodels} ecosystem.\nIn this activity we’ll revisit the Scooby Doo data set and use it to build a KNN model that predicts whether a monster is real or not. Our goal is to select the “best” value for the number of neighbors,k. To begin, let’s load in the data and create our training and testing splits:\n\nscooby &lt;- readr::read_csv(\"scooby.csv\", col_types=\"ddfcd\")\nset.seed(1234)\n\nscooby_split &lt;- initial_split(scooby, \n                              prop = 0.75, \n                              strata = monster_real)\nscooby_train &lt;- training(scooby_split)\nscooby_test &lt;- testing(scooby_split)"
  },
  {
    "objectID": "materials/4_knn_tune_2.html#tuning-a-knn-model",
    "href": "materials/4_knn_tune_2.html#tuning-a-knn-model",
    "title": "Parameter Tuning: A KNN Example",
    "section": "Tuning a KNN Model",
    "text": "Tuning a KNN Model\nTo tune a model, we must first define our model, which includes specifying the recipe and the model specification, and then combining these components into a workflow. There is one key change required, however, to perform parameter tuning: we need to use the tune() function to indicate which parameter we want to tune in our model specification. In the case of KNN, we set neighbors = tune() rather than setting it to a specific value.\n\n# Recipe definition\nknn_recipe &lt;- recipe(monster_real ~ imdb + year_aired, data = scooby_train) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_naomit(all_predictors())\n\n# Model specification\nknn_spec &lt;- nearest_neighbor(neighbors = tune()) |&gt;\n  set_mode(\"classification\") |&gt;\n  set_engine(\"kknn\")\n\n# Workflow definition\nknn_wflow &lt;- workflow() |&gt;\n  add_recipe(knn_recipe) |&gt;\n  add_model(knn_spec)\n\nNext, we need to create our cross-validation splits. In this example, let’s use 5-fold cross-validation.\n\n# Cross-validation splits\nscooby_folds &lt;- vfold_cv(scooby_train, v = 5, strata = monster_real)\n\nWe also need to create a grid of values for the neighbors parameter, for example from 1 to 21 (odds only), using the grid_regular() function. We then pass this grid to the tune_grid() function along with our workflow and cross-validation folds. When tuning a single parameter you could also create a tibble with the values you want to test, but using grid_regular() is a convenient way to generate a range of values and makes tuning multiple parameters easier.\n\nk_grid &lt;- grid_regular(neighbors(range = c(1, 21)), levels = 10)\n\nNote: {tidymodels} does have functionality to automatically generate candidate parameter grids for many models; however, it’s better to manually specify the values when you are first learning to reinforce your understanding of the parameters and their impact on model performance.\nWith the grid defined, we can now perform the tuning process by passing in the workflow, the cross-validation folds, and the grid of values to the tune_grid() function. We also specify the metrics we will use to evaluate the model’s performance, such as accuracy, sensitivity, and specificity. Recall that we use the metric_set() to combine multiple metrics into a single object that can be passed to the tuning function.\n\nscooby_tune &lt;- tune_grid(\n  knn_wflow,\n  resamples = scooby_folds,\n  grid = k_grid,\n  metrics = metric_set(accuracy, sens, spec)\n)\n\nTo collect the results of the tuning process, we use the collect_metrics() function. This returns a tibble with the performance metrics for each value of k that was explored.\n\n(scooby_tune_results &lt;- collect_metrics(scooby_tune))\n\n# A tibble: 30 × 7\n   neighbors .metric  .estimator  mean     n std_err .config              \n       &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1         1 accuracy binary     0.835     5  0.0186 Preprocessor1_Model01\n 2         1 sens     binary     0.917     5  0.0253 Preprocessor1_Model01\n 3         1 spec     binary     0.549     5  0.0267 Preprocessor1_Model01\n 4         3 accuracy binary     0.827     5  0.0172 Preprocessor1_Model02\n 5         3 sens     binary     0.914     5  0.0225 Preprocessor1_Model02\n 6         3 spec     binary     0.525     5  0.0386 Preprocessor1_Model02\n 7         5 accuracy binary     0.843     5  0.0194 Preprocessor1_Model03\n 8         5 sens     binary     0.918     5  0.0199 Preprocessor1_Model03\n 9         5 spec     binary     0.585     5  0.0391 Preprocessor1_Model03\n10         7 accuracy binary     0.875     5  0.0121 Preprocessor1_Model04\n# ℹ 20 more rows\n\n\nYour turn 1: Explore the results of 5-fold cross validation for the various values of k. What is the best value of k based on the accuracy metric? How does this value compare to the sensitivity and specificity metrics?\nIt can be tedious to explore the table of results manually, particularly if you have many parameters to tune. To quickly visualize the results we use the autoplot() function from the {tune} package. This function will create a plot of the performance metrics for each value of our parameters, allowing us to quickly identify the best performing model.\nYour turn 2: Use the autoplot() function to visualize the results of the tuning process. What does the plot tell you about the relationship between the number of neighbors and the model’s performance?\nIf your goal is to select the “best” value of k, you can use the select_best() function to extract the best performing parameter value based on a specific metric. For example, if you want to select the best value of k based on accuracy, you can do the following:\n\nbest_k &lt;- select_best(scooby_tune, metric = \"accuracy\")\n\nThis will return a tibble with the best value of k and its corresponding accuracy.\nYour turn 3: What is the best value of k based on sensitivity? For specificity?\nOnce you have determined what value of k to use, you can finalize your model."
  },
  {
    "objectID": "materials/4_knn_tune_2.html#other-1-se-for-optimizing-parameters",
    "href": "materials/4_knn_tune_2.html#other-1-se-for-optimizing-parameters",
    "title": "Parameter Tuning: A KNN Example",
    "section": "Other 1 SE for optimizing parameters",
    "text": "Other 1 SE for optimizing parameters\nOften there are many values of k that yield similar performance. In these cases, it can be beneficial to select a model that is easier to interpret and less likely to overfit the training data. One common approach is the “one standard error” rule, which selects the simplest model whose performance is within one standard error of the best performing model. This can be done using the select_by_one_std_err() function, which takes the results of tune_grid(), the name of the performance metric (as a character string), and the parameter(s) to optimize (unquoted).\nYour turn 4: What value of k is selected using the one standard error rule based on sensitivity? For specificity?\nYour turn 5: Finalize the model using the value of k selected by the one standard error rule based on accuracy. How does this model perform on the test data compared to the model selected by the best accuracy?"
  },
  {
    "objectID": "materials/4_knn_tune_2.html#usemodels-for-parameter-optimization",
    "href": "materials/4_knn_tune_2.html#usemodels-for-parameter-optimization",
    "title": "Parameter Tuning: A KNN Example",
    "section": "{usemodels} for parameter optimization",
    "text": "{usemodels} for parameter optimization\nIt can be rather tedious to type out all the code required to tune a starting from the recipe. The {usemodels} package implements model skeletons for a few commonly used models, including KNN. Given a formula and data set it will create the recipe, model specification, and workflow for you. It also sets the framework for tuning the model with helpful reminders of the components you should think carefully about.\n\nlibrary(usemodels)\nuse_kknn(monster_real ~ imdb + year_aired, data = scooby)\n\nkknn_recipe &lt;- \n  recipe(formula = monster_real ~ imdb + year_aired, data = scooby) %&gt;% \n  step_zv(all_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) \n\nkknn_spec &lt;- \n  nearest_neighbor(neighbors = tune(), weight_func = tune()) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"kknn\") \n\nkknn_workflow &lt;- \n  workflow() %&gt;% \n  add_recipe(kknn_recipe) %&gt;% \n  add_model(kknn_spec) \n\nset.seed(68778)\nkknn_tune &lt;-\n  tune_grid(kknn_workflow, resamples = stop(\"add your rsample object\"), grid = stop(\"add number of candidate points\"))\n\n\nYour turn 6: What two objects do you need to define in order to run the tuning process using the {usemodels} package? What alterations to the code are needed?"
  },
  {
    "objectID": "materials/3_knn_tune_1.html",
    "href": "materials/3_knn_tune_1.html",
    "title": "Optimizing parameters using resampling",
    "section": "",
    "text": "Uses visualization to see the effect of K in a KNN classification.\nRecognize the need for cross-validation and is able to construct and interact with cross-validation datasets.\nUses cross-validation to evaluate performance metrics across multiple models."
  },
  {
    "objectID": "materials/3_knn_tune_1.html#creating-a-10-fold-cross-validation",
    "href": "materials/3_knn_tune_1.html#creating-a-10-fold-cross-validation",
    "title": "Optimizing parameters using resampling",
    "section": "Creating a 10-fold cross-validation",
    "text": "Creating a 10-fold cross-validation\nLet’s start creating a 10-fold cross-validation data set called scooby_folds by leveraging the function vfold_cv()\n\nset.seed(12345)\n(scooby_folds &lt;- vfold_cv(scooby_train, v = 10))\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits           id    \n   &lt;list&gt;           &lt;chr&gt; \n 1 &lt;split [337/38]&gt; Fold01\n 2 &lt;split [337/38]&gt; Fold02\n 3 &lt;split [337/38]&gt; Fold03\n 4 &lt;split [337/38]&gt; Fold04\n 5 &lt;split [337/38]&gt; Fold05\n 6 &lt;split [338/37]&gt; Fold06\n 7 &lt;split [338/37]&gt; Fold07\n 8 &lt;split [338/37]&gt; Fold08\n 9 &lt;split [338/37]&gt; Fold09\n10 &lt;split [338/37]&gt; Fold10\n\n\nscooby_folds is a tibble with 2 columns (splits and id) and 10 rows. The id column gives us the name of the corresponding fold (or resample), while splits has a more complex datatype containing the training/testing dataset. In the next exercise we will learn a few more details about it.\nYour turn 3:\n\nUse the function get_rsplit() (make sure to view the documentation using ?get_rsplit) to get the second fold from scooby_folds, store the result in scooby_fold_02, and use the function class() (type) of scooby_fold_02.\nHave you seen the class rsplit before?\nWhat is the class of scooby_split? (Remember first code chunk in the Dataset section)\n\nYour turn 4:\n\nUse the testing() and training() functions to extract the testing and training dataset from scooby_fold_02?\nHow big are your training/testing datasets? Does that correspond to the information from the second row of scooby_folds?"
  },
  {
    "objectID": "materials/3_knn_tune_1.html#testing-our-models-using-cross-validation",
    "href": "materials/3_knn_tune_1.html#testing-our-models-using-cross-validation",
    "title": "Optimizing parameters using resampling",
    "section": "Testing our models using cross-validation",
    "text": "Testing our models using cross-validation\nLet’s start by creating a KNN using k = 3\n\nscooby_knn_wf1 &lt;- create_knn_wf(3, scooby_train)\n\nThe function fit_resamples() allows you to fit a workflow using the 10 different training/testing datasets that are stored in scooby_fold. Let’s see it in action:\n\nfit_resamples(scooby_knn_wf1, scooby_folds)\n\n# Resampling results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits           id     .metrics         .notes          \n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;          \n 1 &lt;split [337/38]&gt; Fold01 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [337/38]&gt; Fold02 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [337/38]&gt; Fold03 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [337/38]&gt; Fold04 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [337/38]&gt; Fold05 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [338/37]&gt; Fold06 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [338/37]&gt; Fold07 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [338/37]&gt; Fold08 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [338/37]&gt; Fold09 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [338/37]&gt; Fold10 &lt;tibble [3 × 4]&gt; &lt;tibble [0 × 3]&gt;\n\n\nNotice that the output is 10 by 4 tibble, where three of the columns have nested tibbles inside. An easier way to interact with this dataset is by using the function collect_metrics() which summarizes some performance metrics across our cross-validation dataset:\n\nfit_resamples(scooby_knn_wf1, scooby_folds) |&gt;\n  collect_metrics()\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.848    10  0.0287 Preprocessor1_Model1\n2 brier_class binary     0.118    10  0.0215 Preprocessor1_Model1\n3 roc_auc     binary     0.857    10  0.0331 Preprocessor1_Model1\n\n\nIt seems however that the default metrics don’t include specificity and sensitivity so we will create a new set of metrics using metric_set, which we add as a parameter in fit_resamples()\n\nmulti_metric &lt;- metric_set(accuracy, sens, spec)\nfit_resamples(scooby_knn_wf1, scooby_folds, \n              metrics = multi_metric) |&gt;\n  collect_metrics()\n\n# A tibble: 3 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.848    10  0.0287 Preprocessor1_Model1\n2 sens     binary     0.934    10  0.0209 Preprocessor1_Model1\n3 spec     binary     0.552    10  0.0521 Preprocessor1_Model1\n\n\nYour turn 5: Using your 10-fold cross validations evaluate your accuracy, sensitivity and specificity for KNN models using k = 10, 50, 150.\n\nWhich value would you use to maximize accuracy?\nHow about sensitivity?"
  },
  {
    "objectID": "materials/2_knn_classification.html",
    "href": "materials/2_knn_classification.html",
    "title": "Classification using KNN",
    "section": "",
    "text": "By the end of this activity, you will be able to:\n\nFit a KNN classifier using the {tidymodels} framework\nPredict the class label for a new observation using a fitted KNN classifier\nEvaluate the performance of a KNN classifier using accuracy, sensitivity, and specificity"
  },
  {
    "objectID": "materials/2_knn_classification.html#intended-learning-outcomes",
    "href": "materials/2_knn_classification.html#intended-learning-outcomes",
    "title": "Classification using KNN",
    "section": "",
    "text": "By the end of this activity, you will be able to:\n\nFit a KNN classifier using the {tidymodels} framework\nPredict the class label for a new observation using a fitted KNN classifier\nEvaluate the performance of a KNN classifier using accuracy, sensitivity, and specificity"
  },
  {
    "objectID": "materials/2_knn_classification.html#data-set",
    "href": "materials/2_knn_classification.html#data-set",
    "title": "Classification using KNN",
    "section": "Data set",
    "text": "Data set\nThis activity is inspired by the following blog post from Julia Silge. We will be using a data set collected from the popular animated series, Scooby Doo. Specifically, we’ll build a KNN model to predict whether a monster is real or not.\nTo begin, you can load the data\n\nscooby &lt;- read_csv(\"scooby.csv\", col_types = \"ddfcd\")\n\nand take a glimpse() at it:\n\nglimpse(scooby)\n\nRows: 501\nColumns: 5\n$ year_aired      &lt;dbl&gt; 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, 1969, …\n$ imdb            &lt;dbl&gt; 8.1, 8.1, 8.0, 7.8, 7.5, 8.4, 7.6, 8.2, 8.1, 8.0, 8.5,…\n$ monster_real    &lt;fct&gt; fake, fake, fake, fake, fake, fake, fake, fake, fake, …\n$ title           &lt;chr&gt; \"What a Night for a Knight\", \"A Clue for Scooby Doo\", …\n$ suspects_amount &lt;dbl&gt; 2, 2, 0, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 3, 3, 2, …\n\n\nTeaching note: This is a “clean” version of the data set. If you want to see the original data set, you can load it from the Tidy Tuesday Github Repo. The original data set has some missing values, many logical columns that should be converted to more useful factor variables, and many columns not used in this analysis. If you want to focus on the full data analysis cycle, then you could start with the this raw data set.\nNow that the data set is loaded, let’s split our data set into a training and testing set. We’ll use 75% of the data for training and 25% for testing. We will also stratify our data set by the monster_real column to ensure that the training and testing sets have a similar breakdown of real and fake monsters.\n\n# Be sure to run the setup chunk at the top of the document first!\nset.seed(1234)\nscooby_split &lt;- initial_split(scooby, prop = 0.75, strata = monster_real)\nscooby_train &lt;- training(scooby_split)\nscooby_test &lt;- testing(scooby_split)"
  },
  {
    "objectID": "materials/2_knn_classification.html#classification-and-knn",
    "href": "materials/2_knn_classification.html#classification-and-knn",
    "title": "Classification using KNN",
    "section": "Classification and KNN",
    "text": "Classification and KNN\nWe are interested in determining whether the monster in the episode is real or fake. To do this we will use the year that the episode was aired (year_aired) and the rating the episode got on imdb.\nYour turn 1: Create a scatterplot of imdb_rating vs. year and color the points by real_monster. What do you observe?\nYour turn 2: A KNN classifier assigns a new observation a label based on the labels of the k closest observations using a majority vote. In the scatterplot below there are two new observations marked with a black x. For each of the new observations, determine whether a KNN classifier would label the observation as a real monster or not based on the following values of k:\n\nk = 1\nk = 3\nk = 5\n\nReflection questions:\n\nDid you choose the closest neighbors based on the year_aired or the imdb_rating or a combination of the two?\nWere there any major difficulties in deciding on the closest observations?\n\nYour turn 3: Quantitative variables should be standardized (normalized) before training (fitting) a KNN classifier. Explain in your own words why this is recommended.\n\nA little more data wrangling\nBefore going any farther we should standardize the predictor variables. When making predictions we want to use the same data wrangling steps, and use the mean and standard deviations from the training set to standardize the predictor variables in the testing set to avoid “data leakage”. To do this, we will create a recipe using the {recipes} package from the {tidymodels} framework. A recipe is a way to specify a sequence of data wrangling steps that can be applied to a data set. The recipe will be used to prepare the training data set, and then it will be “baked” to apply the same steps to the testing data set. This ensures that the same transformations are applied to both training and testing data sets, which is crucial for model performance.\nTo begin a recipe, specify a formula that describes the response variable and the predictor variables. The response variable is placed to the left of the ~ and the predictors are placed on the right separated by +. Additional data processing steps are added with step_*() functions. For example, the step_naomit() function allows us to remove rows with missing values and the step_normalize() function allows us to standardize the variables.\n\n# Recipe definition\nknn_recipe &lt;- recipe(monster_real ~ imdb + year_aired, data = scooby_train) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_naomit(all_predictors())\n\nTo view the results, we need to prep() and bake() the recipe. The prep() function prepares the recipe by calculating the necessary statistics (e.g., means and standard deviations) from the training data set, and the bake() function applies the same transformations to the training and testing data sets.\n\nprep(knn_recipe, training = scooby_train) |&gt;\n  bake(new_data = scooby_train)\n\n# A tibble: 375 × 3\n    imdb year_aired monster_real\n   &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt;       \n 1 1.04       -1.57 fake        \n 2 0.636      -1.57 fake        \n 3 1.44       -1.57 fake        \n 4 0.369      -1.57 fake        \n 5 0.904      -1.57 fake        \n 6 1.17       -1.57 fake        \n 7 1.44       -1.57 fake        \n 8 1.04       -1.57 fake        \n 9 1.44       -1.57 fake        \n10 0.904      -1.51 fake        \n# ℹ 365 more rows\n\n\nYour turn 4: Create a scatterplot of imdb_rating vs year and color the points by real_monster using the updated (normalized) scooby_train data set.\n\n\nFitting a KNN classifier in the {tidymodels} framework\nNow that we have our data set ready, and we are familiar with the intuition behind KNN for classification, we can fit a KNN classifier using the {tidymodels} framework. For now, let’s assume we are interested in k = 3 nearest neighbors.\nAs in a KNN regression model we create a model specification using the nearest_neighbor() function. The key difference in the specification for a classification model is that we set the mode to \"classification\" instead of \"regression\".\n\n# Model specification\nknn_spec &lt;- nearest_neighbor(neighbors = 3) |&gt;\n  set_mode(\"classification\") |&gt;\n  set_engine(\"kknn\")\n\nAs with KNN regression we next create a workflow, which combines a model specification and a recipe into a single object.\n\n# Workflow definition\nknn_wflow &lt;- workflow() |&gt;\n  add_recipe(knn_recipe) |&gt;\n  add_model(knn_spec)\n\nFinally, we fit our model to the training set using the fit() function.\n\n# Model training\n(knn_model &lt;- fit(knn_wflow, data = scooby_train))\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_normalize()\n• step_naomit()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nkknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(3,     data, 5))\n\nType of response variable: nominal\nMinimal misclassification: 0.1333333\nBest kernel: optimal\nBest k: 3\n\n\n\n\nMaking predictions and evaluating performance\nTo obtain the predicted class labels we can use the augment() function, which adds columns for the predicted class labels (.pred_class), the predicted probabilities of an observation being in that class (.pred_fake and .pred_real here) to the specified data set.\n\naugment(knn_model, new_data = scooby_test)\n\n# A tibble: 126 × 8\n   .pred_class .pred_fake .pred_real year_aired  imdb monster_real title        \n   &lt;fct&gt;            &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;        &lt;chr&gt;        \n 1 fake                 1          0       1969   8.1 fake         What a Night…\n 2 fake                 1          0       1969   8   fake         Hassle in th…\n 3 fake                 1          0       1969   7.5 fake         Decoy for a …\n 4 fake                 1          0       1969   8.2 fake         Foul Play in…\n 5 fake                 1          0       1969   8.1 fake         The Backstag…\n 6 fake                 1          0       1969   8.5 fake         A Gaggle of …\n 7 fake                 1          0       1970   8.7 fake         A Night of F…\n 8 fake                 1          0       1970   8.3 fake         Nowhere to H…\n 9 fake                 1          0       1970   8.5 fake         Jeepers, It'…\n10 fake                 1          0       1970   7.9 fake         A Tiki Scare…\n# ℹ 116 more rows\n# ℹ 1 more variable: suspects_amount &lt;dbl&gt;\n\n\nYour turn 5: Use the augment() function and fill in the plotting template to create a jittered scatterplot of the predicted probabilities of being a real monster (.pred_real) vs. the predicted probabilities of being a fake monster (.pred_fake). Color the points by monster_real. What do you observe?\nYour turn 6: The below code creates a table of the predicted class labels and the actual class labels. What is the accuracy of your model? Report both the numeric value and give an interpretation of what accuracy means.\nYour turn 7: Sensitivity is the proportion of true positives that are correctly identified (the true positive rate).\n\nWhat is a positive prediction in this KNN model?\nHow do we interpret sensitivity in this context?\nCan you think of an example where sensitivity is more important than accuracy?\nUse the above table to calculate the sensitivity of your model.\n\nYour turn 8: Specificity is the proportion of true negatives that are correctly identified (the true negative rate).\n\nWhat is a negative prediction in this KNN model?\nHow do we interpret specificity in this context?\nCan you think of an example where specificity is more important than accuracy?\nUse the above table to calculate the specificity of your model.\n\nWhile you should be able to calculate accuracy, sensitivity, and specificity “by hand” from a summary table, the {yardstick} package (loaded with {tidymodels}) provides a number of functions to calculate these metrics. For example, the accuracy() function takes a tibble with the predicted class labels and the actual class labels as arguments and returns the accuracy of the model. Similarly, the sensitivity() (also sens()) and specificity() (spec()) functions take a tibble with the predicted class labels and the actual class labels as arguments and return the sensitivity and specificity of the model, respectively.\nRun the below code to check your calculations from above.\n\nscooby_test_aug &lt;- augment(knn_model, new_data = scooby_test)\nscooby_test_aug |&gt; accuracy(truth = monster_real, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.833\n\nscooby_test_aug |&gt; sensitivity(truth = monster_real, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 sensitivity binary         0.939\n\nscooby_test_aug |&gt; specificity(truth = monster_real, estimate = .pred_class)\n\n# A tibble: 1 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 specificity binary         0.464\n\n\nNote: We can also make predictions for all observations in the testing data set using the predict() function. The predict() function takes a fitted model object and a new data set as arguments, and it returns a tibble with the predicted class labels as the only column. While we will typically use augment() to get the predicted class labels along with the original data set, predict() is useful when we only want the predicted class labels.\n\npredict(knn_model, new_data = scooby_test)\n\n# A tibble: 126 × 1\n   .pred_class\n   &lt;fct&gt;      \n 1 fake       \n 2 fake       \n 3 fake       \n 4 fake       \n 5 fake       \n 6 fake       \n 7 fake       \n 8 fake       \n 9 fake       \n10 fake       \n# ℹ 116 more rows"
  },
  {
    "objectID": "materials/2_knn_classification.html#review-exercise",
    "href": "materials/2_knn_classification.html#review-exercise",
    "title": "Classification using KNN",
    "section": "Review exercise",
    "text": "Review exercise\nYou now know the basics of KNN models for classification and how to fit them using the {tidymodels} framework. You also know how to make predictions and evaluate the model performance using accuracy, sensitivity, and specificity. To see if this make sense, complete the following exercise:\nFit a KNN model with k = 5 using imdb, year, and suspects_amount to predict whether a monster is real (monster_real). Calculate the model’s accuracy, sensitivity, and specificity on the test set and write a brief summary of your findings. * How does the model perform compared to the model with k = 3? * What do you think about the number of snacks as a predictor variable? * Do you think KNN is a good model for this data set? Why or why not?"
  },
  {
    "objectID": "materials/2_knn_classification.html#functions-introduced",
    "href": "materials/2_knn_classification.html#functions-introduced",
    "title": "Classification using KNN",
    "section": "Functions introduced",
    "text": "Functions introduced\n\ninitial_split(): Creates a training/testing split of a data set\ntraining(), testing(): Extracts the training and testing data sets from a split\nstep_normalize(all_numeric_predictors()): Standardizes all numeric predictors in a recipe\nstep_naomit(all_predictors()): Removes rows with missing values in a recipe\nprep(), bake(): Prepares a recipe and applies it to a data set\nconf_mat(), accuracy(), sensitivity(), specificity(): Functions to calculate confusion matrix, accuracy, sensitivity, and specificity"
  }
]